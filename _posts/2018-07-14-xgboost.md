---
layout: single
title: "XGBoost: A Scalable Tree Boosting System"
date: 2018-07-14 14:46 -0700
categories: "machine learning"
mathjax: true
---

[Paper Link](http://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf)
## Introduction
- XGBoost is very popular in competitions. The most important factor for its success is its scalability.
- Main contributions
  - Theoretically justified weighted quantile sketch algorithm to compute tree split point proposals.
  - A sparsity-aware algorithm for parallel tree learning.
  - An effective cache-aware block structure for out-of-core tree learning.

## Tree Boosting in a Nutshell
- Tree boosting greedily builds an additive model by adding one tree at each iteration to minimize the loss function.
- This paper considers a second order approximation to the loss function at each greedy iteration for a convex, second-order differentiable loss function and derives the exact formula for evaluating split candidates when each base learner is a tree. 
- The derivation of the exact formula is made possible by the second order approximation, which makes the loss function quadratic.
- XGBoost derives updates by making approximations on the loss functions directly, while the original algorithm tries to find a function that *adjusts* training examples in the right direction.

## Split Finding Algorithms
- The most straight forward algorithm to pick a split point is to look consider all values of a feature as splitting points. This may be expensive when data does not fit into memory.
- As a improvment, the algorithm only proposes split candidates at percentiles. There are two variants: the global variant, where split candidates are proposed per tree, and the local variant, where split candidates are proposed per split. In the local variant, each subtree will have a different set of proposed candidates.
- The second order derivative of the loss function ($$h_i$$) can be interpreted as the importance of an example. We can rewrite the approximated loss functino such that $$h_i$$ is the weight of the squared loss where labels are $$\frac{g_i}{h_i}$$. 
- Real-world features are often sparse due to missing values, zero entries, and artifacts from feature engineering such as one-hot encoding. In fact, we can consider only non-missing and non-zero data as split points and put missing and zero data into one of the groups. Then, we pick the split point according to the loss function.

## System Design
### Column Blocks
- The most time consuming part is to get data into sorted order. XGBoost stores data in [compressed column format](https://www.scipy-lectures.org/advanced/scipy_sparse/csc_matrix.html).
- For the exact greedy algorithm, we need to store the entire dataset in one block and find split points by scanning through pre-sorted entries.
- For the approximate algorithm, we can use multiple blocks (across machines or in the out-of-core setting). Quantile finding becomes a linear scan over the sorted columns. This helps local proposal algorithms.

### Cache Aware Access
- Since values are stored in order of feature values, gradient statistics are stored in non-continuous memory. 
- A buffer is allocated to each thread, which is used to store gradient statistics and aggregate them in a mini-batch manner. This component removes the direct read/write dependency to random memory cells.
- We need to pick the block size such that 1) workload for each thread is large enough compared to the parallization overhead and 2) the gradient statistics of the block will fit into CPU cache to minimize misses.

### Blocks for Out-of-core Computation
- Data is divded into multiple blocks and stored on disk when they do not fit into memory.
- It's important to use an independent thread to pre-fetch the block into memory.
- Two techniques are used.
  - **Block compression**: The block is compressed by columns and decompressed on the fly by an independent thread. This tradesoff computation with disk reading costs.
  - **Block Sharding** Data is sharded to multiple disks in an alternative manner. A pre-fetcher is assigned to each disk, and the training thread reads from each buffer alternatively. This improves throughput when there are multiple disks

## Further Readings
- [Greedy Function Approximation: A Gradient Boosting Machine](https://statweb.stanford.edu/~jhf/ftp/trebst.pdf)
- [From RankNet to LambdaRank to
LambdaMART: An Overview](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.180.634&rep=rep1&type=pdf)
- [Additive Logistic Regression: a Statistical View of Boosting](http://statweb.stanford.edu/~jhf/ftp/boost.pdf)