---
layout: single
title: "Notes on Designing Data-Intensive Applications: Chapter 3 Storage and Retrieval"
date: 2018-08-01 23:41 -0700
categories: "software engineering"
---

## Introduction
- Databases fundamentally does two things: Store data and retrieve the stored data.
- Chapter 3 discusses how data model and queries are interpreted by databases.
- Understanding under-the-hood details can help us pick the right solution and tune the performance.
- We'll first look at two types of storage engines: log-structured and page-oriented storage engines.

## Data Structures That Power Your Database
- Many databases internally uses a log, which is a append-only data file.
- To retrieve data efficiently, we need an index, which is an additional structure derived from primary data and only affects performance of queries.
- Well-chosen indexes speed up queries but slow down writes. Therefore, databases don't index everything by default and requires developers to use knowledge of query pattens to choose index manually.

### Hash Indexes
- Hash Indexes are for key-value data and are similar to a dictionary, which is usually implemented as a hash map (hash table).
- If the database writes only append new entires to a file, the hash table can simply store key to byte offset in the data file. The hash table (with keys) has to fit into memory for quick look up performance, but the values don't have to fit into memory.
- To avoid the disk run out of space, a good solution is to break logs into segments and perform compaction (remove duplicate keys). Further, file segments can be merged while performing compaction. We can use a background thread to perform merging and compaction and switch our read request to the newly created segment when they are read. Afterwards, old segments can be deleted.
- There are a few details for a real implementation of the idea above
  - Use bytes instead of CSV
  - Deletes are adding a special log entry to the data file (tombstone) and the data will be removed during merging and compaction.
  - Index will need to be snapshotted for fast crash recovery (compared to re-indexing).
  - Checksums are required for detecting partially written records.
  - Writes has to strictly be in sequential order. Many implementation choose to have one writer thread.
- Why append-only logs are good
  - Sequential writes are much faster than random writes, especially on magnetic spinning-disk hard drives and to some extent SSDs.
  - Concurrency and crash recovery are much simpler if segment files are append-only or immutable.
  - Merging old segments avoids data fragmentation.
- What are the limitations of hash table indexes?
  - Hash table must fit into memory. If there are too many keys, it will not fit.
  - Range queries are not efficient.


### SSTables and LSM-Trees
#### Constructing and maintaining SSTables
#### Making an LSM-tree out of SSTables
#### Performance optimizations
### B-Trees
#### Making B-Trees reliable
#### B-tree optimizations
### Comparing B-Trees and LSM-Trees
#### Advantages of LSM-trees
#### Downsides of LSM-trees
### Other Indexing Structures
#### Storing values within the index
#### Multi-column indexes
#### Full-text search and fuzzy indexes
#### Keeping everything in memory
## Transaction Processing or Analytics?
### Data Warehousing
#### The divergence between OLTP databases and data warehouses
### Stars and Snowflakes: Schemas for Analytics
## Column-Oriented Storage
### Column Compression
#### Memory bandwidth and vectorized processing
### Sort Order in Column Storage
#### Several different sort orders
### Writing to Column-Oriented Storage
### Aggregation: Data Cubes and Materialized Views
## Summary



Summarized from [Designing Data-Intensive Applications](https://dataintensive.net/).